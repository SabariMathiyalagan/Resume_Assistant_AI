{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "srHfq_jZTwv_"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install unsloth\n",
        "\n",
        "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sgLDM9HETzs5"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 4096\n",
        "dtype = None\n",
        "load_in_4bit = True\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Llama-3.2-3B-Instruct\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wlvRjBgwT3q8"
      },
      "outputs": [],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A3YtCymoT6W5"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, Dataset\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "dataset = load_dataset('yifiyifan/synthetic-resume-fit-labelled')\n",
        "print(type(dataset))\n",
        "\n",
        "\n",
        "label_mapping = {'No Fit': 0, 'Potential Fit': 1, 'Good Fit': 2}\n",
        "dataset = dataset.map(lambda map: {'label': label_mapping[map['label']]})\n",
        "\n",
        "def add_text_field(entry):\n",
        "  entry['text'] = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "\\nYou are an expert resume analyzer. Your job is to analyze both the resume and job description below together and categorize how well the resume fits the job description and give it as a response. Your response should either be 0 1 or 2. 0 means no fit between the resume and job description. 1 means potential fit between the resume and job description, 2 means good fit between the resume and job description.\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "\\nResume: {entry['resume_text']}\n",
        "Job Description:{entry['job_description_text']}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\n",
        "{entry['label']}<|eot_id|>\n",
        "\"\"\"\n",
        "  return entry\n",
        "\n",
        "\n",
        "def remove_outliers(dataset, tokenizer, max_tokens=4096):\n",
        "  def filter_fn(example):\n",
        "        # Tokenize using the tokenizer and count the number of tokens\n",
        "    tokens = tokenizer.encode(example['text'], truncation=False)\n",
        "    return len(tokens) <= max_tokens\n",
        "\n",
        "    # Filter the dataset to keep only examples that don't exceed max_tokens\n",
        "  return dataset.filter(filter_fn)\n",
        "dataset = dataset.map(add_text_field)\n",
        "print(len(dataset['train']))\n",
        "dataset = remove_outliers(dataset, tokenizer)\n",
        "print(len(dataset['train']))\n",
        "\n",
        "\n",
        "df = dataset['train'].to_pandas()\n",
        "min_count = 1100\n",
        "balanced_df = (\n",
        "    df.groupby('label')\n",
        "      .apply(lambda group: group.sample(n=min_count))\n",
        "      .reset_index(drop=True)\n",
        ")\n",
        "df2 = dataset['test'].to_pandas()\n",
        "min_count = 300\n",
        "balanced_df2 = (\n",
        "    df.groupby('label')\n",
        "      .apply(lambda group: group.sample(n=min_count))\n",
        "      .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "new_dataset_train = (Dataset.from_pandas(balanced_df)).shuffle()\n",
        "new_dataset_test = (Dataset.from_pandas(balanced_df2)).shuffle()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1RPbtoInUEOp"
      },
      "outputs": [],
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = new_dataset_train,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False, # Can make training 5x faster for short sequences.\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 170, # Set for 10 percent of total steps\n",
        "        num_train_epochs = 4,\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\", # Use this for WandB etc\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RAVZGJFSxu_p"
      },
      "outputs": [],
      "source": [
        "from unsloth.chat_templates import train_on_responses_only\n",
        "trainer = train_on_responses_only(\n",
        "    trainer,\n",
        "    instruction_part = \"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
        "    response_part = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UD3YQJurxgG4"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "access_token = userdata.get('HF_TOKEN')\n",
        "login(access_token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O1iCA4CJUOLC"
      },
      "outputs": [],
      "source": [
        "trainer_stats = trainer.train()\n",
        "if True:\n",
        "  model.push_to_hub(\"Sabar1/resume_modell2\", token = userdata.get('HF_TOKEN'))\n",
        "  tokenizer.push_to_hub(\"Sabar1/resume_modell2\", token=userdata.get(\"HF_TOKEN\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import PeftModel\n",
        "from transformers import LlamaForCausalLM\n",
        "\n",
        "# Load the base model and PEFT adapters\n",
        "base_model = LlamaForCausalLM.from_pretrained(\"unsloth/Llama-3.2-3B-Instruct\")\n",
        "model = PeftModel.from_pretrained(base_model, \"Sabar1/resume_modell2\")\n",
        "\n",
        "# Merge the adapters with the base model\n",
        "model = model.merge_and_unload()\n",
        "\n",
        "# Save the merged model\n",
        "model.save_pretrained(\"merged_model\")\n",
        "model.push_to_hub(\"Sabar1/merged_model\", token=userdata.get(\"HF_TOKEN\"))"
      ],
      "metadata": {
        "id": "DDOC7FRaybh-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l-p-jnq-TBh3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "# Load the model and tokenizer\n",
        "device = model.device\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "FastLanguageModel.for_inference(model)\n",
        "counter = 0\n",
        "for i in range(900):\n",
        "\n",
        "# Prepare input text\n",
        "  input_text = new_dataset_test[i][\"text\"]\n",
        "\n",
        "  input_text = input_text[:-12]\n",
        "  inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
        "  inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "\n",
        "# Perform inference\n",
        "  with torch.no_grad():\n",
        "      outputs = model.generate(**inputs, max_new_tokens=1)\n",
        "\n",
        "  generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "  original_tokens = inputs[\"input_ids\"][0]\n",
        "  generated_tokens = outputs[0]\n",
        "\n",
        "\n",
        "  new_tokens = generated_tokens[len(original_tokens):]\n",
        "  new_text = tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
        "  print(new_dataset_test[i]['label'])\n",
        "  print(new_text)\n",
        "  print(\"\\n\")\n",
        "  if(new_text == str(new_dataset_test[i]['label'])):\n",
        "    counter += 1\n",
        "print(\"Acc: \", counter/900.0 * 100)\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}